{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Metric and Requirements\n",
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author(s):** kozyr@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started on data, we have to choose our project performance metric and decide the statistical testing criteria.  We'll make use of the metric code we write here when we get to Step 6 (Training) and we'll use the criteria in Step 9 (Testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metric: Accuracy\n",
    "\n",
    "We've picked accuracy as our performance metric.\n",
    "\n",
    "Accuracy $ = \\frac{\\text{correct predictions}}{\\text{total predictions}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric:\n",
    "def get_accuracy(truth, predictions, threshold=0.5, roundoff=2):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    truth: can be Boolean (False, True), int (0, 1), or float (0, 1)\n",
    "    predictions: number between 0 and 1, inclusive\n",
    "    threshold: we convert predictions to 1s if they're above this value\n",
    "    roundoff: report accuracy to how many decimal places?\n",
    "\n",
    "  Returns:\n",
    "    accuracy: number correct divided by total predictions\n",
    "  \"\"\"\n",
    "\n",
    "  truth = np.array(truth) == (1|True)\n",
    "  predicted = np.array(predictions) >= threshold\n",
    "  matches = sum(predicted == truth)\n",
    "  accuracy = float(matches) / len(truth)\n",
    "  return round(accuracy, roundoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out:\n",
    "acc = get_accuracy(truth=[0, False, 1], predictions=[0.2, 0.7, 0.6])\n",
    "print 'Accuracy is ' + str(acc) + '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Loss Function with Performance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(predictions, truth):\n",
    "  # Our methods will be using cross-entropy loss.\n",
    "  return -np.mean(truth * np.log(predictions) + (1 - truth) * np.log(1 - predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some situations:\n",
    "loss = []\n",
    "acc = []\n",
    "for i in range(1000):\n",
    "    for n in [10, 100, 1000]:\n",
    "        p = np.random.uniform(0.01, 0.99, (1, 1))\n",
    "        y = np.random.binomial(1, p, (n, 1))\n",
    "        x = np.random.uniform(0.01, 0.99, (n, 1))\n",
    "        acc = np.append(acc, get_accuracy(truth=y, predictions=x, roundoff=6))\n",
    "        loss = np.append(loss, get_loss(predictions=x, truth=y))\n",
    "\n",
    "df = pd.DataFrame({'accuracy': acc, 'cross-entropy': loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with Seaborn\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.regplot(x=\"accuracy\", y=\"cross-entropy\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing setup:\n",
    "SIGNIFICANCE_LEVEL = 0.05\n",
    "TARGET_ACCURACY = 0.80\n",
    "\n",
    "# Hypothesis test we'll use:\n",
    "from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using standard notation for a one-sided test of one population proportion:\n",
    "n = 100  # Example number of predictions\n",
    "x = 95  # Example number of correct predictions\n",
    "p_value = proportions_ztest(count=x, nobs=n, value=TARGET_ACCURACY, alternative='larger')[1]\n",
    "if p_value < SIGNIFICANCE_LEVEL:\n",
    "  print 'Congratulations! Your model is good enough to build. It passes testing. Awesome!'\n",
    "else:\n",
    "  print 'Too bad.  Better luck next project.  To try again, you need a pristine test dataset.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Get Data\n",
    "\n",
    "This part is done outside Jupyter and run in your VM using the shell script provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Split Data\n",
    "\n",
    "This part is done outside Jupyter and run in your VM using the shell script provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
